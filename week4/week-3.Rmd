---
title: "Week 3"
author: "Santiago Barreda"
output:
  #html_document:
  #  number_sections: true
  #  toc: yes
  #  toc_depth: 3
  #  toc_float:
  #    collapsed: no
  #    smooth_scroll: yes
  pdf_document:
    number_sections: true
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '3'
highlight: tango
---

```{r setup, include=FALSE, echo=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
library(devtools)
source_url("https://raw.githubusercontent.com/santiagobarreda/stats-class/master/data/functions.R")
opts_knit$set(global.par = TRUE)
```

```{r, echo=FALSE}
par(mar =c(4,4,1,1), oma = c(1,0,1,0))  
```

&nbsp; 

# Comparing many groups

Last chapter we talked about comparing two groups. Although it is simple, it is also fundamental and used often: more complicated problems can often be broken down into sets of many two-group questions. However, real experiments don't usually *begin* as two-group questions. 

In the last chapter we found reliable, but noisy, difference between women and girls in average f0. In this chapter, we are going to consider the productions of f0 from all four groups in the data. These research questions would traditionally be addressed with a repeated-measures ANOVA. 

## Data and research questions 

We are still going to work with the Hillenbrand et al. data, and we are going to add a variable indicating 'adultness', and one indicating gender. However, this time we are going to work with all four groups at the same time: `b` (boys), `g` (girls), `m` (men), and `w` (women).

&nbsp; 

```{r, message = FALSE, warning = FALSE}
library (brms)

url1 = "https://raw.githubusercontent.com/santiagobarreda"
url2 = "/stats-class/master/data/h95_vowel_data.csv"
h95 = read.csv (url(paste0 (url1, url2)))
## make variable that indicates if the talker is an adult
h95$adult = ""
h95$adult[h95$type %in% c('w','m')] = "adult"
h95$adult[h95$type %in% c('g','b')] = "child"
## make variable indicating speaker gender
h95$gender = "female"
h95$gender[h95$type %in% c('b','m')] = "male"

## check cross-tabulation of these predictors
table (h95$gender, h95$adult)

## make speaker number a factor
h95$uspeaker = factor (h95$uspeaker)  
```

Our potential research questions are substantially more complicated. Not only are there four groups now, but they also differ along multiple dimensions. This means it is more difficult to make two-group comparisons that ask one single question. For example, the man-girl groups differ according to adultness *and* gender. 

We can consider our data in several ways: as four independent groups, or as two 2-groups comparisons (adult vs child, female vs male). 

```{r, fig.height = 4}
par (mfrow = c(1,3))
boxplot (f0 ~ type, data = h95, main = "Overall", ylim = c(90,330), col = 2:5)
boxplot (f0 ~ adult, data = h95, main = "Adultness", ylim = c(90,330), col = 3:4)
boxplot (f0 ~ gender, data = h95, main = "Gender", ylim = c(90,330), col = 3:4)
```

We are going to focus on the 4-way comparison first. Below, we compare between- and within-speaker variation in f0 by boys (red), girls (green), men (blu), and women (cyan). The figure on the right shows the densities of the overall distributions for each group. 

```{r}
colors = c(2,3,4,5)[ apply (table(h95$uspeaker, h95$type),1,which.max) ]

par (mfrow = c(1,2)); layout (mat = t(c(1,2)), widths = c(.7,.3))
boxplot (f0 ~ uspeaker, data=h95, col = colors, ylim = c(90,330))

## The density figures are rotated 
tmp = density (h95$f0[h95$type=="b"])
plot (tmp$y, tmp$x, lwd = 3, col = 2, ylab = "f0",xlab="Density", 
      ylim = c(90,330), xlim = c(0,0.025), type = 'l')
tmp = density (h95$f0[h95$type=="g"]); lines (tmp$y, tmp$x, lwd = 3, col = 3)
tmp = density (h95$f0[h95$type=="m"]); lines (tmp$y, tmp$x, lwd = 3, col = 4)
tmp = density (h95$f0[h95$type=="w"]); lines (tmp$y, tmp$x, lwd = 3, col = 5)
```

&nbsp; 

## Comparing four (or any number of) groups

We are first going to treat the four groups as four groups with no internal structure.  

&nbsp; 

### The model

We are going to first fit a model that predicts f0 based on speaker type. Unlike in our last example, this is not just a single characteristic that can be represented with a single variable. Actually, the model needs (approximately) one variable per group, but you don't need to worry about this. R treats verbal predictors as 'factors' and assumes that each different label is a different group. Each group of a factor is called a 'level'. Our factor `type` has four levels: `b`,`g`,`m`, and `w`. For models where the predictor is a factor with more than two levels, the model is basically:

$variable \sim group_{[i]}$

where $i$ is a counter variable that goes from 1 to the number of groups, in this case 4. However, we don't usually use a model like this. Instead, just as with the two-group model, the effects associated with different groups are expressed in relation to some reference value. By default, R uses treatment coding which selects a group to be the intercept and then compares each group to this one. In our case our model will use `g` (girls) as the Intercept. This means that our model will return four parameters, the intercept and the difference from each other group mean to the girl's mean. So actually, it is more like: 

$variable \sim Intercept + group_{[i]}$, where $i$ goes from $1...(n-1)$ for $n$ total groups.

&nbsp; 

### Fitting the model and interpreting the results

We can check out the mean and standard deviations for the data to set prior probabilities for the model parameters. 

```{r}
mean (h95$f0)
sd (h95$f0)
```

And fit a model that incorporates this information.  

```{r, eval = FALSE}
options (contrasts = c("contr.treatment","cont.treatment"))
set.seed (1)
model =  
  brm (f0 ~ type + (1|uspeaker), data = h95, chains = 4, cores = 4, 
       warmup = 1000, iter = 6000, thin = 8, 
       prior = c(set_prior("student_t(3, 195, 100)", class = "Intercept"),
                              set_prior("student_t(3, 0, 100)", class = "b"),
                              set_prior("student_t(3, 0, 100)", class = "sd")))
#  saveRDS (model, "model.RDS")       
```
```{r}
## load and inspect pre-fit model
model = readRDS ("model.RDS")
model
```    

We recover the estimated overall mean using the `hypothesis function, and compare this to the actual group:

```{r} 
mean (h95$f0)
hypothesis (model, "(Intercept*4 + typeg + typem + typew)/4 = 0")[[1]][,1:5]
```

We can use the same function to recover the estimated group means, and compare these to the actual group means:

```{r}     
hypothesis (model, c("Intercept = 0",
                     "Intercept + typeg = 0",
                     "Intercept + typem = 0", 
                     "Intercept + typew = 0"))[[1]][,1:5]

tapply (h95$f0, h95$type, mean)
```  

In these Bayesian models, we can actually compare any groups we want in this model, using comparisons of the posterior samples (as shown in chapter 2). For example, the difference between girls and men can be found by asking of one minus the other equals 0 (which would be true if these were identical): 

```{r}     
hypothesis (model, "typeg - typem = 0")[[1]][,1:5]
```

### An alternate approach: Sum coding

Under treatment coding, everything is compared to an arbitrary reference level. This can be bad for more complicated models, because sometimes effects can become difficult to interpret. Researchers often prefer what is called 'sum-'coding'. Under sum coding, the intercept of the model represents the overall average value. Then, the different group effects represent the difference to the overall mean.  
       
Our model still looks like this:

$variable \sim Intercept + group_{[i]}$, where $i$ goes from $1...(n-1)$ for $n$ total groups.

However, the 'missing' group is actually not represented anywhere in the model. The last group from each factor is actually omitted from the model, but it is recoverable based on the effects that *are* present. The omitted level of any factor will have a value equal to:

$-(group_{[i]}+group_{[i+1]}+...+group_{[n-1]})$

That is, the missing effect will be equal to the negative sum of the coefficients that are present. You just ass up all the coefficients you can see and flip the sign on it. By default, R drops the **last** level from your factor. In our case, it will drop the `w` level.  

&nbsp;  

### Fitting the model and interpreting the results: sum coding
   
We fit the same model but with sum coding this time. 

```{r, eval = FALSE}
options (contrasts = c("contr.sum","cont.sum"))
set.seed (1)
model_sum_coding =  
  brm (f0 ~ type + (1|uspeaker), data = h95, chains = 4, cores = 4, 
       warmup = 1000, iter = 6000, thin = 8, 
       prior = c(set_prior("student_t(3, 195, 100)", class = "Intercept"),
                              set_prior("student_t(3, 0, 100)", class = "b"),
                              set_prior("student_t(3, 0, 100)", class = "sd")))
##  saveRDS (model_sum_coding, "model_sum_coding.RDS")       
```
```{r}
## load and inspect pre-fit model
model_sum_coding = readRDS ("model_sum_coding.RDS")
model_sum_coding
```       
    
The Intercept clearly matches our previous mean effect. Since these effects are deviations from the overall mean, we can compare them to the same in our raw data:    
```{r}
hypothesis (model_sum_coding, c("type1 = 0",
                                "type2 = 0",
                                "type3 = 0", 
                                "(type1+type2+type3) = 0"))[[1]][,1:5]

## grop means
tapply (h95$f0, h95$type, mean)
## centered means
tapply (h95$f0, h95$type, mean) - mean (h95$f0)

```

We can compare the effects estimated by the two models in their natural state and they seem quite different. None of the parameters match at all:

```{r}
fixef (model)
fixef (model_sum_coding)
```
 
However, if we transform them appropriately, we can see that they actually are giving us the same information: 
    
```{r}
hypothesis (model, c("(Intercept*4 + typeg + typem + typew)/4 = 0",
                     "Intercept = 0",
                     "Intercept + typeg = 0",
                     "Intercept + typem = 0", 
                     "Intercept + typew = 0"))[[1]][,1:5]

hypothesis (model_sum_coding, c("Intercept = 0",
                                "Intercept + type1 = 0",
                                "Intercept + type2 = 0",
                                "Intercept + type3 = 0", 
                                "Intercept - (type1+type2+type3) = 0"))[[1]][,1:5]
```
    
The choice of model should be based on which fits the research questions. Would you rather have to recover the overall mean of the effect of one group? Is one group specifically logical to use as a reference for the others? Most often, we are interested in differences from the overall behavior. For this, we need to use sum coding. As a result, I am going to focus exclusively on sum coding from here on out. However, just keep in mind that they are interchangeable, and that the results of one will simply be an algebraic manipulation of the other.    
    
  
## Investigating many groups using predictors: Analysis of Variance

### The model

Previously, we just used group labels as predictors. However, we know that our groups have an internal structure such that they differ from each other independently along multiple dimensions (adultness and gender). For example, we might have chosen to fit two separate models that looked like this:

$variable \sim Intercept + gender_{[i]} + error$
$variable \sim Intercept + adult_{[i]} + error$

By the way, since gender and adultness have only two categories, under sum coding the 'missing' coefficient has the same value but opposite sign as the coefficient that is present. this makes it very easy to recover its value!

For several reasons, it is preferable to fit a single model with both predictors at once, rather than fitting two separate models to each research question. The model would then before:

$variable \sim Intercept + gender_{[i]} + adult_{[i]} + error$

Our R model formula wil now look like this, reflecting the use of both predictors simultaneously:

`f0 ~ adult + gender + (1|uspeaker)`

This can be read like "f0 is distributed according to effects for speaker adultness and gender". You may have noticed that our model no longer includes the `type` predictor. This is because the `type` label is perfectly predictable on the basis of `adult` and `gender` (a `g` must have values of `female` and `child`, and so on). Basically, we have decomposed the groups into two components to help us understand the effect of each. This is simply and extension of what we have been doing from the start. For example our model was once: 

$variable \sim Intercept + (type_{[i]}) + error$

However, since type can be exactly represented by combinations of gender and adult, our model sort of always contained this more-complicated model insize it:

$variable \sim Intercept + (gender_{[i]} + adult_{[i]}) + error$

This is what I often refer to as an 'ANOVA-like' decomposition. ANOVA, the ANalysis Of VAriance, is a technique, or a general approach, to understanding data by focusing on the sources of variance contained in it. For example, our model can now explain the variance in our variable attributable to gender, adultness, to between-speaker variation and to production error.

### Fitting the model and interpreting the results

Below I fit the model using the data statistics outlined above.

```{r, eval = FALSE}
set.seed (1)
model_both =  
  brm (f0 ~ adult + gender + (1|uspeaker), data = h95, chains = 4, cores = 4, 
       warmup = 1000, iter = 6000, thin = 8, 
       prior = c(set_prior("student_t(3, 195, 100)", class = "Intercept"),
                              set_prior("student_t(3, 0, 100)", class = "b"),
                              set_prior("student_t(3, 0, 100)", class = "sd"))) 
saveRDS (model_both, "model_both.RDS")   
```
```{r}
## load pre-fit model
model_both = readRDS ("model_both.RDS")
``` 


```{r}
## intercept, boys, girls, men, women
## adult1 = "adult, and gender1="child" because its alphabetical.
## so, plus is adult1, and minus is adult2 since it will be the opposite value
means_pred = hypothesis (model_both, c("Intercept = 0",
                                "Intercept - adult1 - gender1 = 0",
                                "Intercept - adult1 + gender1 = 0",
                                "Intercept + adult1 - gender1 = 0",
                                "Intercept + adult1 + gender1 = 0"))[[1]][,1:5]
means_pred
```

explain the prediction and so on needs to also happen with sum coding
explain posterior predictive check

```{r}
means = tapply (h95$f0, h95$type, mean)
plot (means[1:2], ylim = c(90,330), pch = c('b','g'), type = 'b')
lines (means[3:4], type = 'b', pch = c('m','w'))

lines (means_pred[2:3,2], type = 'b', pch = c('b','g'), col = 4)
lines (means_pred[4:5,2], type = 'b', pch = c('m','w'), col = 4)

aaa = predict (model_both, sample_new_levels = "gaussian")
means_pred2 = tapply (aaa[,1], h95$type, mean)

lines (means_pred2[1:2], pch = c('b','g'), type = 'b',col=2)
lines (means_pred2[3:4], type = 'b', pch = c('m','w'), col=2)

aaaa = predict (model_both, sample_new_levels = "gaussian")
means_pred2 = tapply (aaaa[,1], h95$type, mean)
lines (means_pred2[1:2], pch = c('b','g'), type = 'b',col=3)
lines (means_pred2[3:4], type = 'b', pch = c('m','w'), col=3)

aaaa = predict (model_both, sample_new_levels = "gaussian")
aaaa = predict (model_both, sample_new_levels = "gaussian")


par (mfcol = c(2,3))
boxplot(aaaa[,1] ~ h95$gender, ylim = c(90,320))
boxplot(h95$f0 ~ h95$gender, ylim = c(90,320))

boxplot(aaaa[,1] ~ h95$adult, ylim = c(90,320))
boxplot(h95$f0 ~ h95$adult, ylim = c(90,320))

boxplot(aaaa[,1] ~ h95$type, ylim = c(90,320))
boxplot(h95$f0 ~ h95$type, ylim = c(90,320))

```



```{r}
hypothesis (model_both, c("adult1*2 = 0",
                          "gender1*2 = 0"))
```



options (contrasts = c("contr.sum","cont.sum"))

aa = conditional_effects (model_both)
aa = conditional_effects (model_both)


aaa = predict (model_both)
aaaa = predict (model_both, sample_new_levels = "gaussian")


show that its like the two separate models, but at once
then effects plots
main effect, and plots of the two. paallelism. 
posterior predictive check. bit fit = bad model.


interaction.plot (h95$gender, h95$adult, h95$f0, col = 3:4, 
lwd = 3, type = 'b', pch = c(16,17), cex = 1.5)


interaction.plot (h95$gender, h95$adult, aaaa[,1], col = 3:4, 
lwd = 3, type = 'b', pch = c(16,17), cex = 1.5)



interaction.plot (h95$gender, h95$adult, aaa[,1], col = 3:4, 
lwd = 3, type = 'b', pch = c(16,17), cex = 1.5)


boxplot (aaa[,1] ~ h95$uspeaker)


```{r, eval = FALSE}
set.seed (1)
model_adult =  
  brm (f0 ~ adult + (1|uspeaker), data = h95, chains = 4, cores = 4, 
       warmup = 1000, iter = 6000, thin = 8, 
       prior = c(set_prior("student_t(3, 195, 100)", class = "Intercept"),
                              set_prior("student_t(3, 0, 100)", class = "b"),
                              set_prior("student_t(3, 0, 100)", class = "sd")))
saveRDS (model_adult, "model_adult.RDS")             
```
```{r}
## load pre-fit model
model_adult = readRDS ("model_adult.RDS")
```        

```{r, eval = FALSE}
set.seed (1)
model_gender =  
  brm (f0 ~ gender + (1|uspeaker), data = h95, chains = 4, cores = 4, 
       warmup = 1000, iter = 6000, thin = 8, 
       prior = c(set_prior("student_t(3, 195, 100)", class = "Intercept"),
                              set_prior("student_t(3, 0, 100)", class = "b"),
                              set_prior("student_t(3, 0, 100)", class = "sd"))) 
saveRDS (model_gender, "model_gender.RDS")   
```
```{r}
## load pre-fit model
model_gender = readRDS ("model_gender.RDS")
```   
       













```{r, eval = FALSE}
set.seed (1)
model_interaction =  
  brm (f0 ~ adult * gender + (1|uspeaker), data = h95, chains = 4, cores = 4, 
       warmup = 1000, iter = 6000, thin = 8, 
       prior = c(set_prior("student_t(3, 195, 100)", class = "Intercept"),
                              set_prior("student_t(3, 0, 100)", class = "b"),
                              set_prior("student_t(3, 0, 100)", class = "sd"))) 
saveRDS (model_interaction, "model_interaction.RDS")   
```
```{r}
## load pre-fit model
model_interaction = readRDS ("model_interaction.RDS")
``` 



